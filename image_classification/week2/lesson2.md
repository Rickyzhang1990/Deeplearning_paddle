## 本节知识点  
卷积神经网络的一般结构  
数据增强  
dropout  
感受野  
小卷积核的优势  
1*1的卷积核  
## LeNET  
手写数字识别问题   
第一次5*5  六个kernel 

## Alexnet  
2012视觉识别挑战赛冠军  
使用了两个GPU并行  
8层的网络  
现在池化的step = stride   
局部响应归一化  
模型贡献：  
1、ReLU非线性单元  
2、多GPU训练  
3、局部响应归一化  
4、重叠的池化层  
5、数据增强：水平翻转，平移 ，随机裁剪，色彩抖动，加入噪点 
6、dropout 
## ZFnet   
使用了一个GPU操作  
## VGG   
2014年获得了定位任务的冠军和分类的亚军  
在深度上进行了探索
--更深的网络
--更小的卷积核和池化核  
VGG16和VGG19  
卷积核3*3  步长stride设置为一，paddle设置为1，所有隐藏层都使用Relu激活函数
池化  max pooling 池化层2*2 stride为1 padding设置为0   
感受野：在卷积神经网络中，决定某一层输出结果中一个元素所对应输出层的区域大小，被称作感受野（receptive field) 
## 小卷积核的优势   
一个7*7的卷积核可以用三个串联的3*3的卷积核代替  
一个5*5的卷积核可以用两个串联的3*3的卷积核代替  
参数变少，非线性增强，层级变高，提取更加复杂的特征  
## GoogLenet  
增加网络宽度和深度   
1*1 卷积核  
减少维度，提高非线性  升维适配网络  
## 正则化  
正则化技术是用来解决模型过拟合问题的  
L1 L2正则化，对权重施加约束  
L2约束 L1约束  
L2正则化：计算效率高（有解析解），非稀疏输出 ，无特征选择   
L1正则化：在非稀疏情形下计算效率地 ，稀疏输出，内置特征选择   
 