# 主要内容
理论课程 + 代码实践 
## 人工智能分类   
1、ANI：弱人工智能
2、AGI：通用人工智能  
3、ASI：超人工智能
## 机器学习   
监督学习：通过有标签的训练集
无监督学习：无标签数据集自动发掘模式
增强学习（强化学习）：有反馈的不断学习
深度学习：语音识别，计算机视觉，自然语言处理  
深度神经网络基础：
DNN：深度神经网络  CNN：卷积神经网络  RNN：循环神经网络  
神经网络是模仿人的神经网络而形成的一种学习方法，生物神经网络最小单元是`神经元`，而人工神经网络的最小单位则为感知机，现在也叫神经元。  
## 深度神经网络（DNN)  
神经元的输入为向量，输出为标量，神经元内部含有两个组成：线性变换和非线性变化（激活函数）   
怎样看待神经网络的层数？   
输入的一层为输入层，不计入计算的网络层数。  
训练的三部曲：正向传播，反向传播，梯度下降。  
正向传播：起点是数据，终点是预测值   
反向传播：通过损失函数，反向求每个神经元的偏导数，通过梯度下降算法，不断更新每个神经元的超参数，使得损失函数到达极小值。  
神经元内部：  
**y  = wx + b**（w可以为向量） w为权重，b为神经元的偏置通过激活函数，将输出转化为标量。  
## 激活函数   
sigmoid函数（logistics函数）：计算量打，容易出现梯度消失   
Tanh函数（双曲正切函数）：计算量大，梯度消失问题   
Relu函数：max(0,x) 大于0的部分为输出本身，小于0的部分为0。对梯度消失有收敛作用，只需要一个阈值就可以得到激活值，节省计算量。改进leackRelu.  
`多分类问题`   
Softmax(激活函数）
## 小结  
什么是样本，什么是标签
需要学习的参数（权值，偏置）
激活函数的选择。  
## 反向传播  
起点：损失函数   
损失函数选择：常用 均方误差函数（RSE）适用于线性回归    
交叉熵损失函数：适用于分类问题  
导数：在二维平面是沿该点的切线方向，偏导数是固定一面的方向，梯度是一个方向向量。  
深度学习的最终目标，损失函数的最小值 。
梯度下降的基本过程：  
1、初始化所有神经元的w和b   
2、得到损失函数  
3、得到所有参数的偏导数   
4、得到学习率a  
5、更新w和b
超参数：通产需要程序员自己设置的，比如学习率，batch的值，训练次数 
梯度下降：批量梯度下降（BGD） ，随机梯度下降（SGD）小批量梯度下降（MBGD）。
对于每次batch的设置：通常设置为2的n次方个合适，学习率一般可以选择0.1,0.01,0.001等 。
