## 复习  
全连接可以处理图像分类，但是会出现参数爆炸问题  
pooling:max pooling and  avrage  pooling   
Alex net :5层卷积 + 3层FC ,两块GPU进行训练  
ZFNET :  
VGG 全部使用3\*3的卷积核和2\*2的池化核  
激活函数全部使用了Relu  
Relu适用于网络结构较大   
GoogLeNET在宽度上进行了探索  
1*1卷积核的广泛使用----GoogleNet  
--跨通道信息交互  
--降维（减少参数）  
--升维（用最少参数拓宽网络的channel)  
--增加非线性（把网络做的更深，提高网络的能力）  
## 梯度消失和梯度爆炸   
批归一化（Batch Normalization)  
归一化发生在输入数据之前  
批归一化在层与层之间进行归一化操作  
作用：  
--缓解梯度消失问题  
--加快收敛速度  

意义：  
--使得训练深层网络模型更加容易和稳定  

江湖地位：  
目前BN已经成为所有神经网络的标配技巧  
预测时求得均值和方差：训练阶段时期积累的均值和方差  
## Resnet  
直接堆叠网络效果更差：  
原因：会遇到梯度弥散和梯度爆炸问题   
shortcut的连接方式  
H(x) = F(x) + x   
Resnet将链式法则中的连乘变成了连加   
bottleneck design  
深层网络使用了bottleneck的结构  
Resnet解决了网络退化问题   
Resnet解决了梯度消失问题  
从Resnet开始，网络普遍使用了Batch Normalization  

## 迁移学习  
现在的工程项目中，很少会从头训练，会使用迁移学习的方法    
什么是迁移学习：
让机器自主的从数据中获取知识，从而应用于新的问题中  
迁移学习：  
机器学习的一个重要分支  
讲已经学习过的知识（模型），迁移应用于新的问题中   
举一反三  
照猫画虎问题   
主要解决的问题：  
钱少大量数据  
大量数据与少标注之间的矛盾  
大数据与弱计算之间的矛盾    
普适化模型与个性化需求之间的矛盾       
源领域  目标领域    
两种方法：`model-base` 和 `fine-tuning` 

新数据集很小，和原始数据集很相似（model-base）  
新数据集很大，并且和原始数据集很相似(fine-tuning)  
新数据集很小，并且和原始数据集不相似（model-base）  
